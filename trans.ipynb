{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下库用于批量打开文件\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 以下库用于特殊字符处理\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# 随机生成器，用于随机选取数据\n",
    "import random\n",
    "\n",
    "# 计时器，用于记录运行时长\n",
    "import time\n",
    "\n",
    "# 深度学习库pytorch\n",
    "import torch\n",
    "\n",
    "# 与神经网络相关的pytorch子模块\n",
    "import torch.nn as nn\n",
    "\n",
    "# 用于绘制损失函数下降曲线\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建\n",
    "\n",
    "## 数据集选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单行诗最大长度\n",
    "MAX_LEN = 64\n",
    "MIN_LEN = 5\n",
    "# 禁用的字符，拥有以下符号的诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']', '？', '；']\n",
    "\n",
    "# 一首诗（一行）对应一个列表的元素\n",
    "poetry = []\n",
    "\n",
    "# 按行读取数据 poetry.txt\n",
    "with open('./poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# 遍历处理每一条数据    \n",
    "for line in lines:\n",
    "    # 利用正则表达式拆分 标题 和 内容\n",
    "    fields = line.split(\":\")\n",
    "    # 跳过异常数据\n",
    "    if len(fields) != 2:\n",
    "        continue\n",
    "    # 得到诗词内容（后面不需要标题）\n",
    "    content = fields[1]\n",
    "    # 过滤数据：跳过内容过长、过短、存在禁用符的诗词\n",
    "    if len(content) > MAX_LEN - 2 or len(content) < MIN_LEN:\n",
    "        continue\n",
    "    if any(word in content for word in DISALLOWED_WORDS):\n",
    "        continue\n",
    "        \n",
    "    poetry.append(content.replace('\\n', '')) # 最后要记得删除换行符\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n",
      "慨然抚长剑，济世岂邀名。星旗纷电举，日羽肃天行。遍野屯万骑，临原驻五营。登山麾武节，背水纵神兵。在昔戎戈动，今来宇宙平。\n",
      "翠野驻戎轩，卢龙转征旆。遥山丽如绮，长流萦似带。海气百重楼，岩松千丈盖。兹焉可游赏，何必襄城外。\n",
      "玄兔月初明，澄辉照辽碣。映云光暂隐，隔树花如缀。魄满桂枝圆，轮亏镜彩缺。临城却影散，带晕重围结。驻跸俯九都，停观妖氛灭。\n",
      "碧原开雾隰，绮岭峻霞城。烟峰高下翠，日浪浅深明。斑红妆蕊树，圆青压溜荆。迹岩劳傅想，窥野访莘情。巨川何以济，舟楫伫时英。\n",
      "春蒐驰骏骨，总辔俯长河。霞处流萦锦，风前漾卷罗。水花翻照树，堤兰倒插波。岂必汾阴曲，秋云发棹歌。\n",
      "current_line_count = 24375\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(poetry[i])\n",
    "    \n",
    "print(f\"current_line_count = {len(poetry)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒 -> 2612\n",
      "随 -> 1036\n",
      "穷 -> 482\n",
      "律 -> 118\n",
      "变 -> 286\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "\n",
    "# 统计词频，利用Counter可以直接按单个字符进行统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低词频的词\n",
    "tokens = [token for token, count in counter.items() if count >= MIN_WORD_FREQUENCY]\n",
    "# 打印一下出现次数前5的字\n",
    "for i, (token, count) in enumerate(counter.items()):\n",
    "    print(token, \"->\",count)\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    词典编码器\n",
    "    \"\"\"\n",
    "    UNKNOWN = \"<unknown>\"\n",
    "    PAD = \"<pad>\"\n",
    "    BOS = \"<bos>\" \n",
    "    EOS = \"<eos>\" \n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # 补上特殊词标记：未知词标记、填充字符标记、开始标记、结束标记\n",
    "        tokens = [Tokenizer.UNKNOWN, Tokenizer.PAD, Tokenizer.BOS, Tokenizer.EOS] + tokens\n",
    "        # 词汇表大小\n",
    "        self.dict_size = len(tokens)\n",
    "        # 生成映射关系\n",
    "        self.token_id = {} # 映射: 词 -> 编号\n",
    "        self.id_token = {} # 映射: 编号 -> 词\n",
    "        for idx, word in enumerate(tokens):\n",
    "            self.token_id[word] = idx\n",
    "            self.id_token[idx] = word\n",
    "        \n",
    "        # 各个特殊标记的编号id，方便其他地方使用\n",
    "        self.unknown_id = self.token_id[Tokenizer.UNKNOWN]\n",
    "        self.pad_id = self.token_id[Tokenizer.PAD]\n",
    "        self.bos_id = self.token_id[Tokenizer.BOS]\n",
    "        self.eos_id = self.token_id[Tokenizer.EOS]\n",
    "    \n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        编号 -> 词\n",
    "        \"\"\"\n",
    "        return self.id_token.get(token_id)\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        词 -> 编号，取不到时给 UNKNOWN\n",
    "        \"\"\"\n",
    "        return self.token_id.get(token, self.unknown_id)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        词列表 -> <bos>编号 + 编号列表 + <eos>编号\n",
    "        \"\"\"\n",
    "        token_ids = [self.bos_id, ] # 起始标记\n",
    "        # 遍历，词转编号\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        token_ids.append(self.eos_id) # 结束标记\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        编号列表 -> 词列表(去掉起始、结束标记)\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in token_ids:\n",
    "            # 跳过起始、结束标记\n",
    "            if idx != self.bos_id and idx != self.eos_id:\n",
    "                tokens.append(self.id_to_token(idx))\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2onehot(word_ids, vocab_size):\n",
    "    onehot_tensor = torch.zeros(len(word_ids), vocab_size)\n",
    "    for i, s in enumerate(word_ids): \n",
    "        onehot_tensor[i, s] = 1\n",
    "    return onehot_tensor\n",
    "\n",
    "def onehot2index(word_ids):\n",
    "    return torch.argmax(word_ids, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "class MyDataset(TensorDataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, v, max_length=64):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length  # 每条数据的最大长度\n",
    "        self.v = v\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        word_ids = self.encode_pad_line(line)\n",
    "        return index2onehot(word_ids, self.v)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def encode_pad_line(self, line):\n",
    "        # 编码\n",
    "        word_ids = self.tokenizer.encode(line)\n",
    "        # 如果句子长度不足max_length，填充PAD\n",
    "        word_ids = word_ids + [self.tokenizer.pad_id] * (self.max_length - len(word_ids))\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型类定义\n",
    "\n",
    "### 嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, v, h):\n",
    "        # 调用父类的初始化方法，所有子类均需要该操作\n",
    "        super().__init__()\n",
    "        # 在子类初始化声明中需要定义其包含哪些基本层\n",
    "        self.embedding = nn.Linear(v, h)\n",
    "        self.h = h\n",
    "        self.v = v\n",
    "\n",
    "    def forward(self, onehot_tensor):\n",
    "        # 在forward方法中，我们声明输入张量如何经过这些基本层得到输出张量。\n",
    "        return self.embedding(onehot_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, h, dropout=0.1, max_len=200):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, h)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, h, 2).float() * (-math.log(10000.0) / h))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) \n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, h, a, dropout=0.1, type = 'self'):\n",
    "        '''\n",
    "        h: 嵌入层维度\n",
    "        a: 注意力头数\n",
    "        d_k: 每个注意力头的第二个维度\n",
    "\n",
    "        X: (s,h) ---Wq,Wk,Wv: (h, h//a) ---> Q,K,V: (s, h//a) \n",
    "            ---> softmax(Q*K.t / sqrt(d_k)) * V: (s, h//a)\n",
    "            ---> output: (s, h)\n",
    "        '''\n",
    "        super().__init__()  # 注意这里的修正，使用super()而不是super.__init__()\n",
    "        self.h = h\n",
    "        self.a = a\n",
    "        self.d_k = h // a\n",
    "        self.types = type\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 初始化Q, K, V的权重矩阵\n",
    "        # 每个权重矩阵的维数是(s, h//a) 这里是(h, h)，是将每个头的相应矩阵拼接到一起了\n",
    "        self.Wq = nn.Linear(h, h)\n",
    "        self.Wk = nn.Linear(h, h)\n",
    "        self.Wv = nn.Linear(h, h)\n",
    "        \n",
    "        # 缩放因子，用于缩放点积结果\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, x, y = None, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, s, h)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \"\"\"\n",
    "        Step #1 通过线性变换得到Q, K, V\n",
    "        q,k,v: (batch_size, s, h) ---> (batch_size, s, a, d_k) ---> (batch_size, a, s, d_k)\n",
    "        \"\"\"\n",
    "        if self.types == 'self':            # 自注意力机制，均来自输入x            \n",
    "            assert y is None, (\"Self Attention but different input for Q K V\")\n",
    "            q = k = v = x\n",
    "        elif self.types == 'cross':         # 交叉注意力机制，q来自x，k v来自y\n",
    "            assert y is not None, (\"Cross Attention but the same input for Q K V\")\n",
    "            q = x\n",
    "            k = v = y\n",
    "        else: raise ValueError(\"Undefined Attention Type\")\n",
    "\n",
    "        q = self.Wq(q).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        k = self.Wk(k).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        v = self.Wv(v).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Step#2 计算注意力分数\n",
    "        k: (batch_size, a, s, d_k) ---> (batch_size, a, d_k, s)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        # if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.h)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, h, hiddenDim = None, outDim = None, dropout = 0.1, type = 'relu'):\n",
    "        \"\"\"\n",
    "        x: (h, h) ---> x * W_1: (h, hiddenDim) ---> relu/gelu: (h, hiddenDim) ---> A' * W2: (h, outDim)\n",
    "        W1: (h, hiddenDim)\n",
    "        W2: (hiddenDim, outDim)\n",
    "        默认hiddenDim = 4 * h, outDim = h\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        if hiddenDim is None: hiddenDim = 4 * h\n",
    "        if outDim is None: outDim = h\n",
    "        self.W1 = nn.Linear(h, hiddenDim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.types = type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        if self.types == 'relu': x = F.relu(x)\n",
    "        elif self.types == 'gelu': x = F.gelu(x)\n",
    "        else: raise ValueError(\"Unsupported activation type\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.W2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.normalized_shape, self.weight, self.bias, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder和Decoder\n",
    "\n",
    "包含从嵌入层进入attention block之后的所有流程：encoder decoder feedforward add&norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, h, a, num_encoder_layers, num_decoder_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([Attention(h, a, dropout) \\\n",
    "                                       for _ in range(num_encoder_layers)])\n",
    "        self.encoder_ff = FeedForward(h, dropout=dropout)\n",
    "        # self.encoder_ff = FeedForward(h, dim_feedforward, dropout=dropout)\n",
    "        self.encoder_norm = LayerNorm((h,))\n",
    "        \n",
    "        self.decoders = nn.ModuleList([Attention(h, a, dropout) \\\n",
    "                                       for _ in range(num_decoder_layers)])\n",
    "        self.cross_attention = Attention(h, a, dropout, type = 'cross')\n",
    "        self.decoder_ff = FeedForward(h, dropout=dropout)\n",
    "        # self.decoder_ff = FeedForward(h, dim_feedforward, dropout=dropout)\n",
    "        self.decoder_norm = LayerNorm((h,))\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, src_mask=None, tgt_mask=None):\n",
    "        # print(src_mask, tgt_mask)\n",
    "        for enc in self.encoders:\n",
    "            encoder_input = enc(encoder_input, mask=src_mask)\n",
    "        encoder_input = self.encoder_norm(self.encoder_ff(encoder_input) + encoder_input) # add&norm\n",
    "        \n",
    "        for dec in self.decoders:\n",
    "            decoder_input = dec(decoder_input, mask=tgt_mask)\n",
    "            decoder_input = self.cross_attention(decoder_input, encoder_input, encoder_input)\n",
    "        decoder_input = self.decoder_norm(self.decoder_ff(decoder_input) + decoder_input) # add&norm\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, h, v):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(h, v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.w(x), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, v, h, a, num_encoder_layers, num_decoder_layers, dimFF, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = Embedding(v, h)\n",
    "        self.pos_encoder = PositionalEncoding(h, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(h, dropout)\n",
    "        self.transformer = TransformerEncoderDecoder(h, a, num_encoder_layers, num_decoder_layers, dimFF, dropout)\n",
    "        self.predict = Prediction(h, v)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding.h)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.embedding.h)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.predict(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        key_padding_mask = torch.zeros(tokens.size())\n",
    "        key_padding_mask[tokens == Tokenizer.PAD] = float('-inf')\n",
    "        return key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和预测\n",
    "\n",
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(\n",
      "    (embedding): Linear(in_features=3428, out_features=64, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (pos_decoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoderDecoder(\n",
      "    (encoders): ModuleList(\n",
      "      (0): Attention(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (1): Attention(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (encoder_ff): FeedForward(\n",
      "      (W1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (W2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "    (encoder_norm): LayerNorm()\n",
      "    (decoders): ModuleList(\n",
      "      (0): Attention(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (1): Attention(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cross_attention): Attention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (decoder_ff): FeedForward(\n",
      "      (W1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (W2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "    (decoder_norm): LayerNorm()\n",
      "  )\n",
      "  (predict): Prediction(\n",
      "    (w): Linear(in_features=64, out_features=3428, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(tokens)\n",
    "v = len(tokenizer)\n",
    "batch_size = 8\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = MyDataset(poetry, tokenizer, v)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "\n",
    "h = 64\n",
    "a = 4\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(v, h, a, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train... [epoch 1/50, loss 7.97296]:   2%|▏         | 58/3047 [00:03<02:43, 18.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\1734645290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdata_progress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_progress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 随机选一个位置，拆分src和tgt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\631576261.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_pad_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mindex2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\3098163739.py\u001b[0m in \u001b[0;36mindex2onehot\u001b[1;34m(word_ids, vocab_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mindex2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0monehot_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0monehot_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0monehot_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "import tqdm\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    data_progress = tqdm.tqdm(dataloader, desc=\"Train...\")\n",
    "    for step, data in enumerate(data_progress, 1):\n",
    "        data = data.to(DEVICE)\n",
    "        # 随机选一个位置，拆分src和tgt\n",
    "        e = random.randint(1, 20)\n",
    "        src = data[:, :e]\n",
    "        # tgt不要最后一个token，tgt_y不要第一个的token\n",
    "        tgt, tgt_y = data[:, e:-1], data[:, e + 1:]\n",
    "        # 进行Transformer的计算和预测\n",
    "        out = model(src, tgt)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), tgt_y.contiguous().view(-1, tgt_y.size(-1)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 更新训练进度\n",
    "        data_progress.set_description(f\"Train... [epoch {epoch}/{num_epochs}, loss {(total_loss / step):.5f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\1726809906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msrc_decode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPAD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"src = {src}, src_decode = {src_decode}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtgt_decode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPAD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\1134076983.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# 跳过起始、结束标记\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbos_id\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3116\\1134076983.py\u001b[0m in \u001b[0;36mid_to_token\u001b[1;34m(self, token_id)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0m编号\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid_token\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    word_ids = tokenizer.encode(\"清明时节\")\n",
    "    word_ids = index2onehot(word_ids, v)\n",
    "    src = word_ids[:-2].view(1, -1, v)\n",
    "    tgt = word_ids[-2:-1].view(1, -1, v)\n",
    "    # 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "    for i in range(64):\n",
    "        out = model(src, tgt)\n",
    "        # 预测结果，只需最后一个词\n",
    "        predict = out[:,-1:,:]\n",
    "        # 找出最大值的index并转换位onehot编码，和之前的结果拼到一起\n",
    "        y = torch.argmax(predict, dim=2)\n",
    "        next = torch.zeros(1,1,v)\n",
    "        next[0,0,y] = 1\n",
    "        # 和之前的预测结果拼接到一起\n",
    "        tgt = torch.cat([tgt, next], dim=1)\n",
    "\n",
    "        # 如果为<eos>\n",
    "        if y == tokenizer.eos_id:\n",
    "            break\n",
    "\n",
    "    src_decode = \"\".join([w for w in tokenizer.decode(src[0].tolist()) if w != Tokenizer.PAD])\n",
    "    print(f\"src = {src}, src_decode = {src_decode}\")\n",
    "    tgt_decode = \"\".join([w for w in tokenizer.decode(tgt[0].tolist()) if w != Tokenizer.PAD])\n",
    "    print(f\"tgt = {tgt}, tgt_decode = {tgt_decode}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
