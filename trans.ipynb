{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import random\n",
    "\n",
    "# 深度学习库pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 用于绘制损失函数下降曲线\n",
    "from matplotlib import pyplot as plt\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建\n",
    "\n",
    "## 数据集选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单行诗最大长度\n",
    "MAX_LEN = 64\n",
    "MIN_LEN = 5\n",
    "# 禁用的字符，拥有以下符号的诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']', '？', '；']\n",
    "\n",
    "# 一首诗（一行）对应一个列表的元素\n",
    "poetry = []\n",
    "\n",
    "# 按行读取数据 poetry.txt\n",
    "with open('./poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# 遍历处理每一条数据    \n",
    "for line in lines:\n",
    "    # 利用正则表达式拆分 标题 和 内容\n",
    "    fields = line.split(\":\")\n",
    "    # 跳过异常数据\n",
    "    if len(fields) != 2:\n",
    "        continue\n",
    "    # 得到诗词内容（后面不需要标题）\n",
    "    content = fields[1]\n",
    "    # 过滤数据：跳过内容过长、过短、存在禁用符的诗词\n",
    "    if len(content) > MAX_LEN - 2 or len(content) < MIN_LEN:\n",
    "        continue\n",
    "    if any(word in content for word in DISALLOWED_WORDS):\n",
    "        continue\n",
    "        \n",
    "    poetry.append(content.replace('\\n', '')) # 最后要记得删除换行符\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n",
      "慨然抚长剑，济世岂邀名。星旗纷电举，日羽肃天行。遍野屯万骑，临原驻五营。登山麾武节，背水纵神兵。在昔戎戈动，今来宇宙平。\n",
      "翠野驻戎轩，卢龙转征旆。遥山丽如绮，长流萦似带。海气百重楼，岩松千丈盖。兹焉可游赏，何必襄城外。\n",
      "玄兔月初明，澄辉照辽碣。映云光暂隐，隔树花如缀。魄满桂枝圆，轮亏镜彩缺。临城却影散，带晕重围结。驻跸俯九都，停观妖氛灭。\n",
      "碧原开雾隰，绮岭峻霞城。烟峰高下翠，日浪浅深明。斑红妆蕊树，圆青压溜荆。迹岩劳傅想，窥野访莘情。巨川何以济，舟楫伫时英。\n",
      "春蒐驰骏骨，总辔俯长河。霞处流萦锦，风前漾卷罗。水花翻照树，堤兰倒插波。岂必汾阴曲，秋云发棹歌。\n",
      "current_line_count = 24375\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(poetry[i])\n",
    "    \n",
    "print(f\"current_line_count = {len(poetry)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒 -> 2612\n",
      "随 -> 1036\n",
      "穷 -> 482\n",
      "律 -> 118\n",
      "变 -> 286\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "\n",
    "# 统计词频，利用Counter可以直接按单个字符进行统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低词频的词\n",
    "tokens = [token for token, count in counter.items() if count >= MIN_WORD_FREQUENCY]\n",
    "# 打印一下出现次数前5的字\n",
    "for i, (token, count) in enumerate(counter.items()):\n",
    "    print(token, \"->\",count)\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    词典编码器\n",
    "    \"\"\"\n",
    "    UNKNOWN = \"<unknown>\"\n",
    "    PAD = \"<pad>\"\n",
    "    BOS = \"<bos>\" \n",
    "    EOS = \"<eos>\" \n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # 补上特殊词标记：未知词标记、填充字符标记、开始标记、结束标记\n",
    "        tokens = [Tokenizer.UNKNOWN, Tokenizer.PAD, Tokenizer.BOS, Tokenizer.EOS] + tokens\n",
    "        # 词汇表大小\n",
    "        self.dict_size = len(tokens)\n",
    "        # 生成映射关系\n",
    "        self.token_id = {} # 映射: 词 -> 编号\n",
    "        self.id_token = {} # 映射: 编号 -> 词\n",
    "        for idx, word in enumerate(tokens):\n",
    "            self.token_id[word] = idx\n",
    "            self.id_token[idx] = word\n",
    "        \n",
    "        # 各个特殊标记的编号id，方便其他地方使用\n",
    "        self.unknown_id = self.token_id[Tokenizer.UNKNOWN]\n",
    "        self.pad_id = self.token_id[Tokenizer.PAD]\n",
    "        self.bos_id = self.token_id[Tokenizer.BOS]\n",
    "        self.eos_id = self.token_id[Tokenizer.EOS]\n",
    "    \n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        编号 -> 词\n",
    "        \"\"\"\n",
    "        return self.id_token.get(token_id)\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        词 -> 编号，取不到时给 UNKNOWN\n",
    "        \"\"\"\n",
    "        return self.token_id.get(token, self.unknown_id)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        词列表 -> <bos>编号 + 编号列表 + <eos>编号\n",
    "        \"\"\"\n",
    "        token_ids = [self.bos_id, ] # 起始标记\n",
    "        # 遍历，词转编号\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        token_ids.append(self.eos_id) # 结束标记\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        编号列表 -> 词列表(去掉起始、结束标记)\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in token_ids:\n",
    "            # 跳过起始、结束标记\n",
    "            if idx != self.bos_id and idx != self.eos_id:\n",
    "                tokens.append(self.id_to_token(idx))\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2onehot(word_ids, vocab_size):\n",
    "    if word_ids.dim() == 1:\n",
    "        # 一维情况：(seq_len,)\n",
    "        onehot_tensor = torch.zeros(len(word_ids), vocab_size)\n",
    "        for i, s in enumerate(word_ids): \n",
    "            onehot_tensor[i, s] = 1\n",
    "    elif word_ids.dim() == 2:\n",
    "        # 二维情况：(batch_size, seq_len)\n",
    "        batch_size, seq_len = word_ids.size()\n",
    "        onehot_tensor = torch.zeros(batch_size, seq_len, vocab_size, dtype=torch.float32)\n",
    "        onehot_tensor.scatter_(2, word_ids.unsqueeze(2), 1)\n",
    "    else:\n",
    "        raise ValueError(\"word_ids must be a 1D or 2D tensor\")\n",
    "    return onehot_tensor\n",
    "\n",
    "def onehot2index(word_ids):\n",
    "    return torch.argmax(word_ids, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "class MyDataset(TensorDataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, v, max_len=64):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len  # 每条数据的最大长度\n",
    "        self.v = v\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        word_ids = self.encode_pad_line(line)\n",
    "        return torch.tensor(word_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def encode_pad_line(self, line):\n",
    "        # 编码\n",
    "        word_ids = self.tokenizer.encode(line)\n",
    "        # 如果句子长度不足max_length，填充PAD\n",
    "        if len(word_ids) <= self.max_len:\n",
    "            word_ids = word_ids + [self.tokenizer.pad_id] * (self.max_len - len(word_ids))\n",
    "        else:\n",
    "            word_ids = word_ids[:self.max_len]\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型类定义\n",
    "\n",
    "### 嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, v, h, max_len, padidx = 0):\n",
    "        # 调用父类的初始化方法，所有子类均需要该操作\n",
    "        super().__init__()\n",
    "        # 在子类初始化声明中需要定义其包含哪些基本层\n",
    "        self.embedding = nn.Linear(v, h)\n",
    "        self.h = h\n",
    "        self.v = v\n",
    "        self.max_len = max_len\n",
    "        self.padidx = padidx\n",
    "\n",
    "    def forward(self, src):\n",
    "        # print(src.size())\n",
    "        src = self.key_padding(src, self.max_len)\n",
    "        onehot_tensor = index2onehot(src, self.v)\n",
    "        # print(onehot_tensor.size())\n",
    "        # 在forward方法中，我们声明输入张量如何经过这些基本层得到输出张量。\n",
    "        return self.embedding(onehot_tensor)\n",
    "\n",
    "    def key_padding(self, tokens, max_len = 64):\n",
    "        # 如果句子长度不足max_length，填充PAD        \n",
    "        tokens = F.pad(tokens, (self.padidx, (max_len - tokens.size()[1] - 1)))\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, h, dropout=0.1, max_len=200):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, h)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, h, 2).float() * (-math.log(10000.0) / h))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.size(), self.pe.size())\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, h, a, dropout=0.1, type = 'self'):\n",
    "        '''\n",
    "        h: 嵌入层维度\n",
    "        a: 注意力头数\n",
    "        d_k: 每个注意力头的第二个维度\n",
    "\n",
    "        X: (s,h) ---Wq,Wk,Wv: (h, h//a) ---> Q,K,V: (s, h//a) \n",
    "            ---> softmax(Q*K.t / sqrt(d_k)) * V: (s, h//a)\n",
    "            ---> output: (s, h)\n",
    "        '''\n",
    "        super().__init__()  # 注意这里的修正，使用super()而不是super.__init__()\n",
    "        self.h = h\n",
    "        self.a = a\n",
    "        self.d_k = h // a\n",
    "        self.types = type\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 初始化Q, K, V的权重矩阵\n",
    "        # 每个权重矩阵的维数是(s, h//a) 这里是(h, h)，是将每个头的相应矩阵拼接到一起了\n",
    "        self.Wq = nn.Linear(h, h)\n",
    "        self.Wk = nn.Linear(h, h)\n",
    "        self.Wv = nn.Linear(h, h)\n",
    "        \n",
    "        # 缩放因子，用于缩放点积结果\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, x, y = None, padding_mask=None, tgt_sequence_mask = None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, s, h)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \"\"\"\n",
    "        Step #1 通过线性变换得到Q, K, V\n",
    "        q,k,v: (batch_size, s, h) ---> (batch_size, s, a, d_k) ---> (batch_size, a, s, d_k)\n",
    "        \"\"\"\n",
    "        if self.types == 'self':            # 自注意力机制，均来自输入x            \n",
    "            assert y is None, (\"Self Attention but different input for Q K V\")\n",
    "            q = k = v = x\n",
    "        elif self.types == 'cross':         # 交叉注意力机制，q来自x，k v来自y\n",
    "            assert y is not None, (\"Cross Attention but the same input for Q K V\")\n",
    "            q = x\n",
    "            k = v = y\n",
    "        else: raise ValueError(\"Undefined Attention Type\")\n",
    "\n",
    "        q = self.Wq(q).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        k = self.Wk(k).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        v = self.Wv(v).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Step#2 计算注意力分数\n",
    "        x: (batch_size, s, h)\n",
    "        k: (batch_size, a, s, d_k) ---> (batch_size, a, d_k, s)\n",
    "        tgt_sequence_mask: (s, s) ---> (batch_size, a, s, s)\n",
    "        padding_mask : (batch_size, s) ---> (batch_size, a, s, s)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        if padding_mask is not None:\n",
    "            mask = padding_mask.view(batch_size, 1, 1, x.size(1)).expand(batch_size, self.a, x.size(1), x.size(1))\n",
    "            if tgt_sequence_mask is not None: \n",
    "                s_mask = tgt_sequence_mask.view(1, 1, x.size(1), x.size(1)).   \\\n",
    "                expand(batch_size, self.a, -1, -1)\n",
    "                mask = s_mask.logical_or(mask)\n",
    "            # assert self.types == 'self'\n",
    "            # print(mask.size(), scores.size())\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.h)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, h, hiddenDim = None, outDim = None, dropout = 0.1, type = 'relu'):\n",
    "        \"\"\"\n",
    "        x: (h, h) ---> x * W_1: (h, hiddenDim) ---> relu/gelu: (h, hiddenDim) ---> A' * W2: (h, outDim)\n",
    "        W1: (h, hiddenDim)\n",
    "        W2: (hiddenDim, outDim)\n",
    "        默认hiddenDim = 4 * h, outDim = h\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        if hiddenDim is None: hiddenDim = 4 * h\n",
    "        if outDim is None: outDim = h\n",
    "        self.W1 = nn.Linear(h, hiddenDim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.types = type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        if self.types == 'relu': x = F.relu(x)\n",
    "        elif self.types == 'gelu': x = F.gelu(x)\n",
    "        else: raise ValueError(\"Unsupported activation type\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.W2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.normalized_shape, self.weight, self.bias, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder和Decoder\n",
    "\n",
    "包含从嵌入层进入attention block之后的所有流程：encoder decoder feedforward add&norm\n",
    "对于encoder来讲，self-attention ---> add&norm ---> feedforward ---> add&norm\n",
    "对于decoder来讲, self-attention ---> add&norm ---> cross-attention ---> add&norm ---> feedforward ---> add&norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, h, a, num_encoder_layers, num_decoder_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(h, a, dropout),\n",
    "                LayerNorm((h,)),\n",
    "                FeedForward(h, dropout = dropout),\n",
    "                LayerNorm((h,))\n",
    "            ]) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(h, a, dropout),\n",
    "                LayerNorm((h,)),\n",
    "                Attention(h, a, dropout, type='cross'),\n",
    "                LayerNorm((h,)),\n",
    "                FeedForward(h, dropout = dropout),\n",
    "                LayerNorm((h,))\n",
    "            ]) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, src_padding_mask=None, tgt_padding_mask = None, tgt_sequence_mask=None):\n",
    "        global DEVICE        \n",
    "        # print(src_padding_mask, tgt_sequence_mask)\n",
    "        for enc in self.encoders:\n",
    "            attention, norm1, ff, norm2 = enc\n",
    "            encoder_input = norm1(attention(encoder_input, padding_mask=src_padding_mask) + encoder_input)\n",
    "            encoder_input = norm2(ff(encoder_input) + encoder_input)\n",
    "\n",
    "        for dec in self.decoders:\n",
    "            self_attention, norm1, cross_attention, norm2, ff, norm3 = dec\n",
    "            decoder_input = norm1(self_attention(decoder_input, padding_mask=tgt_padding_mask, \\\n",
    "                                                 tgt_sequence_mask = tgt_sequence_mask) + decoder_input)\n",
    "            decoder_input = norm2(cross_attention(decoder_input, encoder_input, \\\n",
    "                                                  padding_mask=tgt_padding_mask, tgt_sequence_mask = tgt_sequence_mask) + decoder_input)\n",
    "            decoder_input = norm3(ff(decoder_input) + decoder_input)\n",
    "        return decoder_input        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, h, v):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(h, v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.w(x), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, v, h, a, num_encoder_layers, num_decoder_layers, dimFF, dropout, max_len, padidx):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(v, h, max_len, padidx)\n",
    "        self.pos_encoder = PositionalEncoding(h, dropout, max_len)\n",
    "        self.pos_decoder = PositionalEncoding(h, dropout, max_len)\n",
    "        self.transformer = TransformerEncoderDecoder(h, a, num_encoder_layers, num_decoder_layers, dimFF, dropout)\n",
    "        self.predict = Prediction(h, v)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask = None, tgt_padding_mask = None, tgt_sequence_mask = None):\n",
    "        # print(src.size())\n",
    "        if src_padding_mask is None: \n",
    "            src_padding_mask = self.get_key_padding_mask(src).to(DEVICE)\n",
    "        if tgt_padding_mask is None: \n",
    "            tgt_padding_mask = self.get_key_padding_mask(tgt).to(DEVICE)\n",
    "        if tgt_sequence_mask is None: \n",
    "            tgt_sequence_mask = self.get_sequence_mask().to(DEVICE)\n",
    "        # print(src_padding_mask.size(), tgt.size())\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding.h)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.embedding.h)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        output = self.transformer(src, tgt, src_padding_mask, tgt_padding_mask, tgt_sequence_mask)\n",
    "        output = self.predict(output)\n",
    "        return output\n",
    "    \n",
    "    # @staticmethod\n",
    "    def get_sequence_mask(self):\n",
    "        size = self.max_len\n",
    "        return torch.triu(torch.full((size, size), True, device=DEVICE), diagonal=1)\n",
    "    \n",
    "    # @staticmethod\n",
    "    def get_key_padding_mask(self, tokens):\n",
    "        key_padding_mask = torch.zeros(tokens.size())\n",
    "        key_padding_mask[tokens == Tokenizer.PAD] = True\n",
    "        key_padding_mask = F.pad(key_padding_mask, (0, (self.max_len - tokens.size()[1])), \"constant\", True)\n",
    "        # print(f\"keypaddingmasksize{key_padding_mask}\")\n",
    "        return key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和预测\n",
    "\n",
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(\n",
      "    (embedding): Linear(in_features=3428, out_features=128, bias=True)\n",
      "  )\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (pos_decoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoderDecoder(\n",
      "    (encoders): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (decoders): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "        (4): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (5): LayerNorm()\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "        (4): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (5): LayerNorm()\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): LayerNorm()\n",
      "        (2): Attention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (3): LayerNorm()\n",
      "        (4): FeedForward(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (5): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predict): Prediction(\n",
      "    (w): Linear(in_features=128, out_features=3428, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(tokens)\n",
    "v = len(tokenizer)\n",
    "batch_size = 64\n",
    "max_len = 64\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = MyDataset(poetry, tokenizer, v, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "h = 128\n",
    "a = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 4 * h\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(v, h, a, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, max_len, tokenizer.pad_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train... [epoch 1/5, loss 8.13977]:   0%|          | 0/381 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "import tqdm\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    data_progress = tqdm.tqdm(dataloader, desc=\"Train...\")\n",
    "    for step, data in enumerate(data_progress, 1):\n",
    "        data = data.to(DEVICE)\n",
    "        # data: (batch_size, seq_len)\n",
    "        # 随机选一个位置，拆分src和tgt\n",
    "        # print(data.size())\n",
    "        e = random.randint(1, 20)\n",
    "        src = data[:, :e]\n",
    "        # print(src.size())\n",
    "        # tgt不要最后一个token，tgt_y不要第一个的token\n",
    "        tgt, tgt_y = data[:, e:-1], data[:, e + 1:]\n",
    "        # 进行Transformer的计算和预测 out:(batch_size, max_len, v)\n",
    "        out = model(src, tgt)\n",
    "        # print(out.size(), tgt_y.size()) \n",
    "        # 将tgt_y 转化为与out形状相同的变量\n",
    "        tgt_y = F.pad(tgt_y, (0, (max_len - tgt_y.size()[1])), \"constant\", tokenizer.pad_id)\n",
    "        tgt_y = index2onehot(tgt_y, v)\n",
    "        # print(tgt_y.size())\n",
    "        # 在forward方法中，我们声明输入张量如何经过这些基本层得到输出张量。\n",
    "        loss = criterion(out.view(-1, out.size(-1)), tgt_y.contiguous().view(-1, tgt_y.size(-1)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 更新训练进度\n",
    "        data_progress.set_description(f\"Train... [epoch {epoch}/{num_epochs}, loss {(total_loss / step):.5f}]\")\n",
    "        break\n",
    "    break\n",
    "# print(src_padding_mask.size(), tgt.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = tensor([[  2, 403, 235, 293]]), src_decode = 清明时\n",
      "tgt = tensor([[197,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1]]), tgt_decode = 节\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    word_ids = torch.tensor(tokenizer.encode(\"清明时节\"))\n",
    "    # word_ids = index2onehot(word_ids, v)\n",
    "    # word_ids: (batch_size, max_len)\n",
    "    src = word_ids[:-2].view(1, -1)\n",
    "    tgt = word_ids[-2:-1].view(1, -1)\n",
    "    # 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "    for i in range(64):\n",
    "        out = model(src, tgt)\n",
    "        # 预测结果，只需最后一个词\n",
    "        predict = out[:,-1:,:]\n",
    "        # 找出最大值的index并转换为onehot编码，和之前的结果拼到一起\n",
    "        y = torch.argmax(predict, dim=2)\n",
    "        next = torch.zeros(1,1,v)\n",
    "        next[0,0,y] = 1\n",
    "        next = onehot2index(next)\n",
    "        # 和之前的预测结果拼接到一起\n",
    "        tgt = torch.cat([tgt, next], dim=1)\n",
    "\n",
    "        # 如果为<eos>\n",
    "        if y == tokenizer.eos_id:\n",
    "            break\n",
    "        if y in [tokenizer.pad_id, tokenizer.unknown_id]:\n",
    "            continue\n",
    "    \n",
    "    # src = onehot2index(src)\n",
    "    # tgt = onehot2index(tgt)\n",
    "    src_decode = \"\".join([w for w in tokenizer.decode(src[0].tolist()) if w not in [Tokenizer.PAD, Tokenizer.UNKNOWN]])\n",
    "    print(f\"src = {src}, src_decode = {src_decode}\")\n",
    "    tgt_decode = \"\".join([w for w in tokenizer.decode(tgt[0].tolist()) if w not in [Tokenizer.PAD, Tokenizer.UNKNOWN]])\n",
    "    print(f\"tgt = {tgt}, tgt_decode = {tgt_decode}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
