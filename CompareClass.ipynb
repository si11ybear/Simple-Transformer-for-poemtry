{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import random\n",
    "\n",
    "# 深度学习库pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 用于绘制损失函数下降曲线\n",
    "from matplotlib import pyplot as plt\n",
    "%pdb off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, h, a, dropout=0, type = 'self'):\n",
    "        '''\n",
    "        h: 嵌入层维度\n",
    "        a: 注意力头数\n",
    "        d_k: 每个注意力头的第二个维度\n",
    "\n",
    "        X: (s,h) ---Wq, Wk, Wv: (h, h//a) ---> Q,K,V: (s, h//a) \n",
    "            ---> softmax(Q * K.t / sqrt(d_k)) * V: (s, h//a)\n",
    "            ---> output: (s, h)\n",
    "        '''\n",
    "        super().__init__()  # 注意这里的修正，使用super()而不是super.__init__()\n",
    "        self.h = h\n",
    "        self.a = a\n",
    "        self.d_k = h // a\n",
    "        self.types = type\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 初始化Q, K, V的权重矩阵\n",
    "        # 每个权重矩阵的维数是(s, h//a) 这里是(h, h)，是将每个头的相应矩阵拼接到一起了\n",
    "        self.Wq = nn.Linear(h, h)\n",
    "        self.Wk = nn.Linear(h, h)\n",
    "        self.Wv = nn.Linear(h, h)\n",
    "        \n",
    "        # 缩放因子，用于缩放点积结果\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "        self.out_proj = nn.Linear(h, h)\n",
    "\n",
    "    def forward(self, x, y = None, padding_mask=None, tgt_sequence_mask = None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, s = tgt_s, h)\n",
    "        y: (batch_size, s = src_s, h)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \"\"\"\n",
    "        Step #1 通过线性变换得到Q, K, V\n",
    "        q,k,v: (batch_size, s, h) ---> (batch_size, s, a, d_k) ---> (batch_size, a, s, d_k)\n",
    "        crros attention时q的s=tgt_s, kv的s=src_s\n",
    "        \"\"\"\n",
    "        if self.types == 'self':            # 自注意力机制，均来自输入x            \n",
    "            assert y is None, (\"Self Attention but different input for Q K V\")\n",
    "            q = k = v = x\n",
    "        elif self.types == 'cross':         # 交叉注意力机制，q来自x，k v来自y\n",
    "            assert y is not None, (\"Cross Attention but the same input for Q K V\")\n",
    "            q = x\n",
    "            k = v = y\n",
    "        else: raise ValueError(\"Undefined Attention Type\")\n",
    "\n",
    "        q = self.Wq(q).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        k = self.Wk(k).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "        v = self.Wv(v).view(batch_size, -1, self.a, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Step#2 计算注意力分数\n",
    "        x: (batch_size, s, h)\n",
    "        k: (batch_size, a, src_s, d_k) ---> (batch_size, a, d_k, src_s)\n",
    "        tgt_sequence_mask: (tgt_s, tgt_s) ---> (batch_size, a, tgt_s, tgt_s)\n",
    "        padding_mask : (batch_size, src_s) ---> (batch_size, a, tgt_s, src_s)\n",
    "        \"\"\"\n",
    "        k_len  = k.size()[2]\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        # if padding_mask is not None:\n",
    "        #     # print(padding_mask)\n",
    "        #     mask = padding_mask.view(batch_size, 1, 1, k_len).expand(batch_size, self.a, q.size()[2], k_len)\n",
    "        #     if tgt_sequence_mask is not None: \n",
    "        #         assert self.types == 'self' , \\\n",
    "        #                 (f\"Only Self Attention in Decoder Needs Sequence Mask, but now {self.types} attetion!\")\n",
    "        #         s_mask = tgt_sequence_mask.view(1, 1, k_len, k_len).   \\\n",
    "        #         expand(batch_size, self.a, -1, -1)\n",
    "        #         mask = s_mask.logical_or(mask)\n",
    "        #     # print(mask.size(), scores.size())\n",
    "        #     # print(mask)\n",
    "        #     scores = scores.masked_fill(mask, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.h)\n",
    "        output = self.out_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512]) torch.Size([2, 10, 512])\n",
      "Custom Attention Output: tensor([[[ 7.6254e-02, -2.3000e-01, -2.4884e-01,  ..., -1.2974e-01,\n",
      "          -2.4446e-01, -5.3609e-02],\n",
      "         [ 7.6644e-02, -2.2916e-01, -2.4818e-01,  ..., -1.2940e-01,\n",
      "          -2.4590e-01, -5.2144e-02],\n",
      "         [ 7.7572e-02, -2.2836e-01, -2.4693e-01,  ..., -1.3126e-01,\n",
      "          -2.4442e-01, -5.2186e-02],\n",
      "         ...,\n",
      "         [ 7.7370e-02, -2.2816e-01, -2.4882e-01,  ..., -1.2853e-01,\n",
      "          -2.4481e-01, -5.3064e-02],\n",
      "         [ 7.5168e-02, -2.2928e-01, -2.4806e-01,  ..., -1.2686e-01,\n",
      "          -2.4602e-01, -5.6681e-02],\n",
      "         [ 7.5953e-02, -2.2779e-01, -2.4856e-01,  ..., -1.2830e-01,\n",
      "          -2.4703e-01, -5.0346e-02]],\n",
      "\n",
      "        [[ 5.1680e-02, -2.7051e-01, -2.0929e-01,  ..., -1.6202e-01,\n",
      "          -1.7383e-01, -9.3966e-04],\n",
      "         [ 5.0170e-02, -2.7117e-01, -2.0826e-01,  ..., -1.6102e-01,\n",
      "          -1.7110e-01,  6.9406e-04],\n",
      "         [ 5.3044e-02, -2.7202e-01, -2.0860e-01,  ..., -1.6277e-01,\n",
      "          -1.7140e-01, -2.8374e-04],\n",
      "         ...,\n",
      "         [ 5.2693e-02, -2.6949e-01, -2.0834e-01,  ..., -1.6440e-01,\n",
      "          -1.7265e-01,  9.0566e-04],\n",
      "         [ 5.3434e-02, -2.7157e-01, -2.0673e-01,  ..., -1.6352e-01,\n",
      "          -1.7248e-01, -1.5250e-03],\n",
      "         [ 5.2181e-02, -2.7175e-01, -2.0783e-01,  ..., -1.6180e-01,\n",
      "          -1.7159e-01, -1.9417e-04]]], grad_fn=<ViewBackward0>)\n",
      "Standard Attention Output: tensor([[[ 7.6254e-02, -2.3000e-01, -2.4884e-01,  ..., -1.2974e-01,\n",
      "          -2.4446e-01, -5.3609e-02],\n",
      "         [ 7.6644e-02, -2.2916e-01, -2.4818e-01,  ..., -1.2940e-01,\n",
      "          -2.4590e-01, -5.2144e-02],\n",
      "         [ 7.7572e-02, -2.2836e-01, -2.4693e-01,  ..., -1.3126e-01,\n",
      "          -2.4442e-01, -5.2186e-02],\n",
      "         ...,\n",
      "         [ 7.7370e-02, -2.2816e-01, -2.4882e-01,  ..., -1.2853e-01,\n",
      "          -2.4481e-01, -5.3064e-02],\n",
      "         [ 7.5168e-02, -2.2928e-01, -2.4806e-01,  ..., -1.2686e-01,\n",
      "          -2.4602e-01, -5.6681e-02],\n",
      "         [ 7.5953e-02, -2.2779e-01, -2.4856e-01,  ..., -1.2830e-01,\n",
      "          -2.4703e-01, -5.0346e-02]],\n",
      "\n",
      "        [[ 5.1680e-02, -2.7051e-01, -2.0929e-01,  ..., -1.6202e-01,\n",
      "          -1.7383e-01, -9.3965e-04],\n",
      "         [ 5.0170e-02, -2.7117e-01, -2.0826e-01,  ..., -1.6102e-01,\n",
      "          -1.7110e-01,  6.9406e-04],\n",
      "         [ 5.3044e-02, -2.7202e-01, -2.0860e-01,  ..., -1.6277e-01,\n",
      "          -1.7140e-01, -2.8373e-04],\n",
      "         ...,\n",
      "         [ 5.2693e-02, -2.6949e-01, -2.0834e-01,  ..., -1.6440e-01,\n",
      "          -1.7265e-01,  9.0567e-04],\n",
      "         [ 5.3434e-02, -2.7157e-01, -2.0673e-01,  ..., -1.6352e-01,\n",
      "          -1.7248e-01, -1.5250e-03],\n",
      "         [ 5.2181e-02, -2.7175e-01, -2.0783e-01,  ..., -1.6180e-01,\n",
      "          -1.7159e-01, -1.9412e-04]]], grad_fn=<TransposeBackward0>)\n",
      "Attention outputs are consistent.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设您的自定义 Attention 类名为 CustomAttention\n",
    "# from your_module import CustomAttention\n",
    "\n",
    "# 定义模型参数\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "\n",
    "# 初始化自定义 Attention 模型\n",
    "custom_attention = Attention(d_model, nhead, type = 'cross')\n",
    "\n",
    "# 初始化标准库 MultiheadAttention 模型\n",
    "standard_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 设置自定义 Attention 的权重和偏置\n",
    "    custom_attention.Wq.weight.copy_(standard_attention.in_proj_weight[:d_model])\n",
    "    custom_attention.Wq.bias.copy_(standard_attention.in_proj_bias[:d_model])\n",
    "    custom_attention.Wk.weight.copy_(standard_attention.in_proj_weight[d_model:2*d_model])\n",
    "    custom_attention.Wk.bias.copy_(standard_attention.in_proj_bias[d_model:2*d_model])\n",
    "    custom_attention.Wv.weight.copy_(standard_attention.in_proj_weight[2*d_model:])\n",
    "    custom_attention.Wv.bias.copy_(standard_attention.in_proj_bias[2*d_model:])\n",
    "    custom_attention.out_proj.weight.copy_(standard_attention.out_proj.weight)\n",
    "    custom_attention.out_proj.bias.copy_(standard_attention.out_proj.bias)\n",
    "\n",
    "# 创建相同的输入数据\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "query = torch.rand(batch_size, seq_len, d_model)\n",
    "key = value = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# 使用自定义 Attention 进行前向传播\n",
    "custom_output = custom_attention(query, key)\n",
    "\n",
    "# 使用标准库 MultiheadAttention 进行前向传播\n",
    "standard_output, _ = standard_attention(query, key, value)\n",
    "\n",
    "print(custom_output.size(), standard_output.size())\n",
    "\n",
    "# 比较输出结果\n",
    "print(\"Custom Attention Output:\", custom_output)\n",
    "print(\"Standard Attention Output:\", standard_output)\n",
    "\n",
    "# 检查输出是否一致\n",
    "if torch.allclose(custom_output, standard_output, atol=1e-5):\n",
    "    print(\"Attention outputs are consistent.\")\n",
    "else:\n",
    "    print(\"Attention outputs are not consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较Layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "    def forward(self, input):\n",
    "        # 计算均值和方差\n",
    "        assert self.normalized_shape[0] == input.size()[-1], (\"Unmatched Shape.\")\n",
    "        mean = input.mean(dim=-1, keepdim=True)\n",
    "        var = input.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # 应用层归一化公式\n",
    "        normalized_input = (input - mean) / std\n",
    "        normalized_input = normalized_input * self.weight + self.bias\n",
    "        \n",
    "        return normalized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom LayerNorm Output: tensor([[[-1.4499,  1.0367,  0.8635,  ...,  1.0624,  1.6254,  1.5586],\n",
      "         [ 1.1919,  0.9680,  0.1012,  ...,  0.4476,  1.0002, -0.9387],\n",
      "         [ 1.2584,  0.4808,  1.4803,  ...,  0.1199,  1.1277, -1.0652],\n",
      "         ...,\n",
      "         [-1.1420,  0.1190,  0.1091,  ...,  0.6674, -0.5147,  0.4453],\n",
      "         [ 1.5784,  0.6634, -0.2846,  ..., -0.4316, -0.8661, -0.3981],\n",
      "         [-0.2079, -1.5761,  0.7877,  ..., -0.3204,  1.3260,  1.7002]],\n",
      "\n",
      "        [[-0.3636, -0.9277,  1.3401,  ...,  1.4149, -1.2291, -0.7934],\n",
      "         [-1.0764,  0.4581,  0.6112,  ..., -0.0618, -1.6071,  1.1913],\n",
      "         [-0.5549,  0.9851, -1.1974,  ...,  1.1155, -0.6968, -1.2720],\n",
      "         ...,\n",
      "         [-0.2990,  0.1023,  1.3759,  ..., -0.7829,  0.4815, -0.5369],\n",
      "         [-1.4962, -0.8674,  1.6019,  ..., -1.1599, -0.6209,  0.6707],\n",
      "         [ 0.1447, -0.1330,  0.5088,  ...,  1.8131, -0.8557, -0.8239]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Standard LayerNorm Output: tensor([[[-1.4499,  1.0367,  0.8635,  ...,  1.0624,  1.6254,  1.5586],\n",
      "         [ 1.1919,  0.9680,  0.1012,  ...,  0.4476,  1.0002, -0.9387],\n",
      "         [ 1.2584,  0.4808,  1.4803,  ...,  0.1199,  1.1277, -1.0652],\n",
      "         ...,\n",
      "         [-1.1420,  0.1190,  0.1091,  ...,  0.6674, -0.5147,  0.4453],\n",
      "         [ 1.5784,  0.6634, -0.2846,  ..., -0.4316, -0.8661, -0.3981],\n",
      "         [-0.2079, -1.5761,  0.7877,  ..., -0.3204,  1.3260,  1.7002]],\n",
      "\n",
      "        [[-0.3636, -0.9277,  1.3401,  ...,  1.4149, -1.2291, -0.7934],\n",
      "         [-1.0764,  0.4581,  0.6112,  ..., -0.0618, -1.6071,  1.1913],\n",
      "         [-0.5549,  0.9851, -1.1974,  ...,  1.1155, -0.6968, -1.2720],\n",
      "         ...,\n",
      "         [-0.2990,  0.1023,  1.3759,  ..., -0.7829,  0.4815, -0.5369],\n",
      "         [-1.4962, -0.8674,  1.6019,  ..., -1.1599, -0.6209,  0.6707],\n",
      "         [ 0.1447, -0.1330,  0.5088,  ...,  1.8131, -0.8557, -0.8239]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "LayerNorm outputs are consistent.\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "normalized_shape = (d_model,)\n",
    "\n",
    "# 初始化自定义 LayerNorm 模型\n",
    "custom_layernorm = LayerNorm(normalized_shape)\n",
    "\n",
    "# 初始化标准库 LayerNorm 模型\n",
    "standard_layernorm = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "# 创建相同的输入数据\n",
    "input_data = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# 使用自定义 LayerNorm 进行前向传播\n",
    "custom_output = custom_layernorm(input_data)\n",
    "\n",
    "# 使用标准库 LayerNorm 进行前向传播\n",
    "standard_output = standard_layernorm(input_data)\n",
    "\n",
    "# 比较输出结果\n",
    "print(\"Custom LayerNorm Output:\", custom_output)\n",
    "print(\"Standard LayerNorm Output:\", standard_output)\n",
    "\n",
    "# 检查输出是否一致\n",
    "if torch.allclose(custom_output, standard_output, atol=1e-6):\n",
    "    print(\"LayerNorm outputs are consistent.\")\n",
    "else:\n",
    "    print(\"LayerNorm outputs are not consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, h, hiddenDim = None, outDim = None, dropout = 0.1, type = 'relu'):\n",
    "        \"\"\"\n",
    "        x: (h, h) ---> x * W_1: (h, hiddenDim) ---> relu/gelu: (h, hiddenDim) ---> A' * W2: (h, outDim)\n",
    "        W1: (h, hiddenDim)\n",
    "        W2: (hiddenDim, outDim)\n",
    "        默认hiddenDim = 4 * h, outDim = h\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        if hiddenDim is None: hiddenDim = 4 * h\n",
    "        if outDim is None: outDim = h\n",
    "        self.W1 = nn.Linear(h, hiddenDim)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.W2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.types = type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        if self.types == 'relu': x = F.relu(x)\n",
    "        elif self.types == 'gelu': x = F.gelu(x)\n",
    "        else: raise ValueError(\"Unsupported activation type\")\n",
    "        # x = self.dropout(x)\n",
    "        x = self.W2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom FeedForward Output: tensor([[[-0.0033,  0.0491, -0.1526,  ...,  0.4070, -0.1007, -0.1621],\n",
      "         [ 0.0931,  0.0313, -0.1054,  ...,  0.4616, -0.0819, -0.0800],\n",
      "         [ 0.1692, -0.0269, -0.1421,  ...,  0.4423, -0.0836, -0.1786],\n",
      "         ...,\n",
      "         [ 0.0956,  0.0691, -0.1539,  ...,  0.3764, -0.0960, -0.1147],\n",
      "         [ 0.0786,  0.1818, -0.1807,  ...,  0.4164, -0.0534, -0.2579],\n",
      "         [-0.0387,  0.0827, -0.1627,  ...,  0.3216,  0.0080, -0.0844]],\n",
      "\n",
      "        [[ 0.0102,  0.1768, -0.1897,  ...,  0.4118, -0.1269, -0.0931],\n",
      "         [ 0.1048,  0.1075, -0.1676,  ...,  0.4553, -0.0972, -0.1229],\n",
      "         [ 0.1263,  0.0266, -0.1519,  ...,  0.5270, -0.0882, -0.1278],\n",
      "         ...,\n",
      "         [ 0.1030, -0.0176, -0.1740,  ...,  0.3288, -0.1698, -0.1251],\n",
      "         [ 0.0236, -0.0760, -0.2338,  ...,  0.4686, -0.1037, -0.0839],\n",
      "         [ 0.0725, -0.0319, -0.1449,  ...,  0.3669, -0.1149, -0.0957]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Standard FeedForward Output: tensor([[[-0.0033,  0.0491, -0.1526,  ...,  0.4070, -0.1007, -0.1621],\n",
      "         [ 0.0931,  0.0313, -0.1054,  ...,  0.4616, -0.0819, -0.0800],\n",
      "         [ 0.1692, -0.0269, -0.1421,  ...,  0.4423, -0.0836, -0.1786],\n",
      "         ...,\n",
      "         [ 0.0956,  0.0691, -0.1539,  ...,  0.3764, -0.0960, -0.1147],\n",
      "         [ 0.0786,  0.1818, -0.1807,  ...,  0.4164, -0.0534, -0.2579],\n",
      "         [-0.0387,  0.0827, -0.1627,  ...,  0.3216,  0.0080, -0.0844]],\n",
      "\n",
      "        [[ 0.0102,  0.1768, -0.1897,  ...,  0.4118, -0.1269, -0.0931],\n",
      "         [ 0.1048,  0.1075, -0.1676,  ...,  0.4553, -0.0972, -0.1229],\n",
      "         [ 0.1263,  0.0266, -0.1519,  ...,  0.5270, -0.0882, -0.1278],\n",
      "         ...,\n",
      "         [ 0.1030, -0.0176, -0.1740,  ...,  0.3288, -0.1698, -0.1251],\n",
      "         [ 0.0236, -0.0760, -0.2338,  ...,  0.4686, -0.1037, -0.0839],\n",
      "         [ 0.0725, -0.0319, -0.1449,  ...,  0.3669, -0.1149, -0.0957]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "FeedForward outputs are consistent.\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "dim_ff = 2048\n",
    "\n",
    "# 初始化自定义 FeedForward 模型\n",
    "custom_feedforward = FeedForward(d_model, dim_ff, dropout=0)\n",
    "\n",
    "# 初始化标准库 FeedForward 模型\n",
    "standard_feedforward = nn.Sequential(\n",
    "    nn.Linear(d_model, dim_ff),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim_ff, d_model)\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 设置第一层的权重和偏置\n",
    "    custom_feedforward.W1.weight.copy_(standard_feedforward[0].weight)\n",
    "    custom_feedforward.W1.bias.copy_(standard_feedforward[0].bias)\n",
    "    # 设置第二层的权重和偏置\n",
    "    custom_feedforward.W2.weight.copy_(standard_feedforward[2].weight)\n",
    "    custom_feedforward.W2.bias.copy_(standard_feedforward[2].bias)\n",
    "\n",
    "\n",
    "# 创建相同的输入数据\n",
    "input_data = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# 使用自定义 FeedForward 进行前向传播\n",
    "custom_output = custom_feedforward(input_data)\n",
    "\n",
    "# 使用标准库 FeedForward 进行前向传播\n",
    "standard_output = standard_feedforward(input_data)\n",
    "\n",
    "# 比较输出结果\n",
    "print(\"Custom FeedForward Output:\", custom_output)\n",
    "print(\"Standard FeedForward Output:\", standard_output)\n",
    "\n",
    "# 检查输出是否一致\n",
    "if torch.allclose(custom_output, standard_output, atol=1e-6):\n",
    "    print(\"FeedForward outputs are consistent.\")\n",
    "else:\n",
    "    print(\"FeedForward outputs are not consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较Transformer架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, h, a, num_encoder_layers, num_decoder_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(h, a, dropout),\n",
    "                LayerNorm((h,)),\n",
    "                FeedForward(h, dropout = dropout),\n",
    "                LayerNorm((h,))\n",
    "            ]) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(h, a, dropout),\n",
    "                LayerNorm((h,)),\n",
    "                Attention(h, a, dropout, type='cross'),\n",
    "                LayerNorm((h,)),\n",
    "                FeedForward(h, dropout = dropout),\n",
    "                LayerNorm((h,))\n",
    "            ]) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, src_padding_mask=None, tgt_padding_mask = None, tgt_sequence_mask=None):\n",
    "\n",
    "        for enc in self.encoders:\n",
    "            attention, norm1, ff, norm2 = enc\n",
    "            encoder_input = norm1(attention(encoder_input, padding_mask=src_padding_mask) + encoder_input)\n",
    "            encoder_input = norm2(ff(encoder_input) + encoder_input)\n",
    "\n",
    "        for dec in self.decoders:\n",
    "            self_attention, norm1, cross_attention, norm2, ff, norm3 = dec\n",
    "            decoder_input = norm1(self_attention(decoder_input, padding_mask=tgt_padding_mask, \\\n",
    "                                                 tgt_sequence_mask = tgt_sequence_mask) + decoder_input)\n",
    "            decoder_input = norm2(cross_attention(decoder_input, encoder_input, \\\n",
    "                                                  padding_mask=src_padding_mask) + decoder_input)\n",
    "            decoder_input = norm3(ff(decoder_input) + decoder_input)\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# 设置全局随机种子\n",
    "# def set_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # 设置随机种子\n",
    "# set_seed(42)\n",
    "\n",
    "# 定义模型参数\n",
    "h = 512\n",
    "a = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# 初始化自定义 Transformer 模型\n",
    "custom_transformer = TransformerEncoderDecoder(h, a, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# 初始化标准库 Transformer 模型\n",
    "standard_transformer = nn.Transformer(d_model=h, nhead=a, num_encoder_layers=num_encoder_layers, \n",
    "                                      num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, \n",
    "                                      dropout=dropout, batch_first=True)\n",
    "\n",
    "# 设置相同的权重和偏置\n",
    "with torch.no_grad():\n",
    "    for i in range(num_encoder_layers):\n",
    "        custom_attention, custom_norm1, custom_ff, custom_norm2 = custom_transformer.encoders[i]\n",
    "        standard_encoder_layer = standard_transformer.encoder.layers[i]\n",
    "        \n",
    "        # Attention weights\n",
    "        custom_attention.q_proj.weight.copy_(standard_encoder_layer.self_attn.in_proj_weight[:h])\n",
    "        custom_attention.q_proj.bias.copy_(standard_encoder_layer.self_attn.in_proj_bias[:h])\n",
    "        custom_attention.k_proj.weight.copy_(standard_encoder_layer.self_attn.in_proj_weight[h:2*h])\n",
    "        custom_attention.k_proj.bias.copy_(standard_encoder_layer.self_attn.in_proj_bias[h:2*h])\n",
    "        custom_attention.v_proj.weight.copy_(standard_encoder_layer.self_attn.in_proj_weight[2*h:])\n",
    "        custom_attention.v_proj.bias.copy_(standard_encoder_layer.self_attn.in_proj_bias[2*h:])\n",
    "        custom_attention.out_proj.weight.copy_(standard_encoder_layer.self_attn.out_proj.weight)\n",
    "        custom_attention.out_proj.bias.copy_(standard_encoder_layer.self_attn.out_proj.bias)\n",
    "        \n",
    "        # LayerNorm weights\n",
    "        custom_norm1.layer_norm.weight.copy_(standard_encoder_layer.norm1.weight)\n",
    "        custom_norm1.layer_norm.bias.copy_(standard_encoder_layer.norm1.bias)\n",
    "        custom_norm2.layer_norm.weight.copy_(standard_encoder_layer.norm2.weight)\n",
    "        custom_norm2.layer_norm.bias.copy_(standard_encoder_layer.norm2.bias)\n",
    "        \n",
    "        # FeedForward weights\n",
    "        custom_ff.linear1.weight.copy_(standard_encoder_layer.linear1.weight)\n",
    "        custom_ff.linear1.bias.copy_(standard_encoder_layer.linear1.bias)\n",
    "        custom_ff.linear2.weight.copy_(standard_encoder_layer.linear2.weight)\n",
    "        custom_ff.linear2.bias.copy_(standard_encoder_layer.linear2.bias)\n",
    "\n",
    "    for i in range(num_decoder_layers):\n",
    "        custom_self_attention, custom_norm1, custom_cross_attention, custom_norm2, custom_ff, custom_norm3 = custom_transformer.decoders[i]\n",
    "        standard_decoder_layer = standard_transformer.decoder.layers[i]\n",
    "        \n",
    "        # Self-Attention weights\n",
    "        custom_self_attention.q_proj.weight.copy_(standard_decoder_layer.self_attn.in_proj_weight[:h])\n",
    "        custom_self_attention.q_proj.bias.copy_(standard_decoder_layer.self_attn.in_proj_bias[:h])\n",
    "        custom_self_attention.k_proj.weight.copy_(standard_decoder_layer.self_attn.in_proj_weight[h:2*h])\n",
    "        custom_self_attention.k_proj.bias.copy_(standard_decoder_layer.self_attn.in_proj_bias[h:2*h])\n",
    "        custom_self_attention.v_proj.weight.copy_(standard_decoder_layer.self_attn.in_proj_weight[2*h:])\n",
    "        custom_self_attention.v_proj.bias.copy_(standard_decoder_layer.self_attn.in_proj_bias[2*h:])\n",
    "        custom_self_attention.out_proj.weight.copy_(standard_decoder_layer.self_attn.out_proj.weight)\n",
    "        custom_self_attention.out_proj.bias.copy_(standard_decoder_layer.self_attn.out_proj.bias)\n",
    "        \n",
    "        # Cross-Attention weights\n",
    "        custom_cross_attention.q_proj.weight.copy_(standard_decoder_layer.multihead_attn.in_proj_weight[:h])\n",
    "        custom_cross_attention.q_proj.bias.copy_(standard_decoder_layer.multihead_attn.in_proj_bias[:h])\n",
    "        custom_cross_attention.k_proj.weight.copy_(standard_decoder_layer.multihead_attn.in_proj_weight[h:2*h])\n",
    "        custom_cross_attention.k_proj.bias.copy_(standard_decoder_layer.multihead_attn.in_proj_bias[h:2*h])\n",
    "        custom_cross_attention.v_proj.weight.copy_(standard_decoder_layer.multihead_attn.in_proj_weight[2*h:])\n",
    "        custom_cross_attention.v_proj.bias.copy_(standard_decoder_layer.multihead_attn.in_proj_bias[2*h:])\n",
    "        custom_cross_attention.out_proj.weight.copy_(standard_decoder_layer.multihead_attn.out_proj.weight)\n",
    "        custom_cross_attention.out_proj.bias.copy_(standard_decoder_layer.multihead_attn.out_proj.bias)\n",
    "        \n",
    "        # LayerNorm weights\n",
    "        custom_norm1.layer_norm.weight.copy_(standard_decoder_layer.norm1.weight)\n",
    "        custom_norm1.layer_norm.bias.copy_(standard_decoder_layer.norm1.bias)\n",
    "        custom_norm2.layer_norm.weight.copy_(standard_decoder_layer.norm2.weight)\n",
    "        custom_norm2.layer_norm.bias.copy_(standard_decoder_layer.norm2.bias)\n",
    "        custom_norm3.layer_norm.weight.copy_(standard_decoder_layer.norm3.weight)\n",
    "        custom_norm3.layer_norm.bias.copy_(standard_decoder_layer.norm3.bias)\n",
    "        \n",
    "        # FeedForward weights\n",
    "        custom_ff.linear1.weight.copy_(standard_decoder_layer.linear1.weight)\n",
    "        custom_ff.linear1.bias.copy_(standard_decoder_layer.linear1.bias)\n",
    "        custom_ff.linear2.weight.copy_(standard_decoder_layer.linear2.weight)\n",
    "        custom_ff.linear2.bias.copy_(standard_decoder_layer.linear2.bias)\n",
    "\n",
    "# 创建相同的输入数据\n",
    "batch_size = 2\n",
    "src_len = 10\n",
    "tgt_len = 10\n",
    "\n",
    "src = torch.rand(batch_size, src_len, h)\n",
    "tgt = torch.rand(batch_size, tgt_len, h)\n",
    "src_padding_mask = torch.zeros(batch_size, src_len).bool()\n",
    "tgt_padding_mask = torch.zeros(batch_size, tgt_len).bool()\n",
    "tgt_sequence_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len)\n",
    "\n",
    "# 使用自定义 Transformer 进行前向传播\n",
    "custom_output = custom_transformer(src, tgt, src_padding_mask, tgt_padding_mask, tgt_sequence_mask)\n",
    "\n",
    "# 使用标准库 Transformer 进行前向传播\n",
    "standard_output = standard_transformer(src, tgt, tgt_mask=tgt_sequence_mask, \n",
    "                                       src_key_padding_mask=src_padding_mask, \n",
    "                                       tgt_key_padding_mask=tgt_padding_mask)\n",
    "\n",
    "# 比较输出结果\n",
    "print(\"Custom Transformer Output:\", custom_output)\n",
    "print(\"Standard Transformer Output:\", standard_output)\n",
    "\n",
    "# 检查输出是否一致\n",
    "if torch.allclose(custom_output, standard_output, atol=1e-6):\n",
    "    print(\"Outputs are consistent.\")\n",
    "else:\n",
    "    print(\"Outputs are not consistent.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
