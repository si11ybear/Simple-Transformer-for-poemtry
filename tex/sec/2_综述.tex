\section{对已有工作的调研}\label{sec-2}
自从2017年Vaswani等人提出的Transformer模型以来，它已经成为了自然语言处理（NLP）和其他序列建模任务的核心架构。本节将基于四篇相关论文，探讨对Transformer进行优化和扩展的研究进展，并结合个人理解提供深入分析。
	
\citeauthor{shazeerGLUVariantsImprove2020}等人在文献\textit{GLU Variants Improve Transformer}中研究了非线性激活函数的选取，提出了一系列基于门控线性单元（GLU）的变体来改善Transformer，比如Swish-GLU、ReGLU等。作者通过对多种NLP任务进行实验，包括语言建模、机器翻译等，验证了这些新激活函数的有效性。结果显示，使用GLU变体的Transformer模型能够在保持甚至提高性能的同时，降低计算成本和内存占用。
	
我们小组认为GLU通过引入门控机制，能够更灵活地调整信息流，从而提高模型表达能力。这种改进使得模型可以更好地适应不同任务的需求，尤其是在需要精细控制信息传递的情况下。我们小组同时认为GLU变体可以在不牺牲性能的前提下简化Transformer架构，还为后续研究提供了有价值的参考。
	
\citeauthor{xiongLayerNormalizationTransformer2020}等人的工作\textit{On Layer Normalization in the Transformer Architecture}聚焦于层归一化（Layer Normalization）对Transformer性能的影响，即LN结构调整。他们的研究表明，在Transformer的不同位置应用层归一化可以显著影响训练稳定性和最终效果。作者发现，将层归一化应用于残差连接之前而非之后，可以加速收敛并提升模型性能。
	
我们小组认为层归一化不仅是一个简单的规范化工具，它实际上可以被视为一种增强特征表示的方法。通过保持每一层输出的稳定性，层归一化可以帮助模型更好地学习到数据中的抽象模式。这对于像Transformer这样依赖于自注意力机制来建模序列间关系的模型尤为重要。同时，考虑到不同任务的需求各异，未来的研究应该探索如何针对特定任务调整层归一化的策略，以获得最佳的效果。例如，在一些需要保留原始输入尺度信息的任务中，可能需要设计新的规范化方法或者调整现有方法的应用方式。
	
我们小组还注意到Xiong等人的工作虽然主要集中在文本处理领域，但其结论对于其他类型的序列数据（如时间序列预测、语音识别等）同样具有指导意义。因此，层归一化以及类似的技术在未来跨领域的深度学习研究中将继续扮演关键角色。
	
\citeauthor{shawSelfAttentionRelativePosition2018}等人在文献\textit{Self-Attention with Relative Position Representations}中介绍了相对位置编码的概念。不同于绝对位置编码，相对位置编码考虑了两个token之间的距离而不是它们的具体位置。通过引入相对位置编码，该方法增强了模型理解文本内部结构的能力，特别是在处理长序列时。此改进有助于保持句子中词语间的位置信息，对于诸如机器翻译、文本摘要等任务至关重要。这种方法不仅简化了模型结构，还增强了对句子内部结构的理解能力。
	
从理论上讲，相对位置表示能够更好地模拟人类阅读文本的方式——我们往往根据上下文环境以及词语间的相对距离来进行理解和推理。实际上，这一改进确实提高了模型在各种NLP任务上的表现，如机器翻译、文本摘要生成等。此外，由于其灵活性和普适性，该方法还可以很容易地应用于其他涉及序列建模的问题域，包括但不限于语音识别、时间序列预测等领域。我们小组认为未来的研究可能会进一步探索如何更有效地结合多种类型的先验知识（如语法结构、语义角色标注等），以期构建出更为强大的语言理解系统。
	
\citeauthor{childGeneratingLongSequences2019}等人提出的文献\textit{Generating Long Sequences with Sparse Transformers}中提出了稀疏注意力机制，介绍了一种称为稀疏Transformer的新架构，它特别适用于生成极长的序列。相比于标准的全连接Transformer，稀疏Transformer利用稀疏性来减少计算复杂度，同时保持甚至提高了生成质量。这种方法允许模型处理比以往更长的输入输出序列，这对于生成式任务如音乐创作、视频描述等具有重大意义。
	
我们小组认为稀疏变换器的关键在于它打破了全连接自注意力的传统，提出了更加灵活且有效的替代方案。这种方法为解决大规模数据集上的长文本生成问题提供了新的思路。此外，稀疏变换器的成功也引发了我们小组对于注意力机制本质的思考：是否真的需要让每个token都与其他所有token建立联系？还是说，存在某种最优的稀疏模式可以让模型既有效又高效地运作？

关于这四篇论文的详细内容，可以阅读第\ref{sec-4}节改进策略。