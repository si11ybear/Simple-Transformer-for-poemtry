\section{总结与反思}\label{sec-8}

\paragraph{丁玺博} 我在本项目中主要负责两篇文献阅读和模型的落地实现部分，整体来说基本完成全部目标。这个过程并不顺利，主要困难是集中在模型的封装调用和改进文本生成逻辑两部分。我清楚认识到一个模型从基本的训练完成到可以自由使用，再到根据实际需求针对性改造与使用，实际上是两个非常艰难、漫长和复杂的过程。特别是结合现实需求的设计，我需要不断去考虑一切可能影响到文本生成的因素，比如模型词向量嵌入步长，src和tgt的动态变化，推理模式随机性的控制等等，并尝试与实际生成古诗文的需求结合起来，比如平仄和韵律。这实在是对一个人耐心和逻辑的极大挑战。如果简单将模型的训练和生成作为后端，用户使用作为前端，那么我确实是对前后端融合以及前端的工作有了更深的认识，这些都是之前仅仅关注模型训练的我所不曾掌握的知识。未来的学习和工作中，我也会更多考虑现实需求和框架融合，而不仅仅关注模型的进步。这学期学习到了非常多有关深度学习的知识，并在手动执行后有了更多的体悟。感谢老师和助教的辛苦付出，遥祝新年快乐，一切顺利。

\paragraph{李健宁} 我在本项目中主要负责精度Transformer原始论文，并仅利用Pytorch基础模块和线性层搭建了自己的Transformer架构。由于之前几乎没有使用Pytorch的经验，在刚开始接触这一部分时第六次作业带给我很大启发。从一开始不清楚初始化和前向传播函数，到能自己完全写出一个模块，在编程过程中进一步感受到了类在编程中的重要作用。在一步一步实现Transformer的过程中，不仅对Transformer的细节有了更加深刻的理解，也对Pytorch中的并行算子。批处理、损失函数等有了更为详细的认知。

代码和算法能力只有在实操中才能得到飞跃，在未来的学习中，我会尝试更多的利用机器学习和深度学习，注重模型的细节和不同模型间的联系，将理论所学与实际操作相结合起来。最后，感谢老师和助教的辛苦付出，让我能深切感受到深度学习的魅力和价值。

\paragraph{肖庆成} 在本项目中，我的主要职责涵盖了两篇关键文献的精读、现有工作的调研、数据集预处理以及将创新性改进应用于自建的Transformer模型。尽管整个过程复杂多变，但总体进展顺利。其中最具有挑战性的环节是将定制化的改进措施融入到手动实现的Transformer架构中，这要求我克服与Python库内置Transformer模型之间的变量命名和函数差异的问题。经过细致的对比和调整，最终成功实现了这一目标。

对于未来的工作，我认为可以在数据集预处理阶段投入更多精力，以期通过引入更丰富的数据资源来进一步优化模型性能。此外，我还意识到，在今后的学习和职业发展中，应更加注重实际应用场景的需求以及不同技术框架间的兼容性，而不仅仅是追求模型算法上的突破。这个学期里，我不仅深入学习了深度学习领域的广泛知识，而且通过实践操作加深了对这些理论的理解。感谢老师和助教团队在整个学期中的悉心指导和支持，祝您们新年快乐，万事如意！

